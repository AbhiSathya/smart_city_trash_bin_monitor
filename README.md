# ğŸš® Smart City Trash Bin Monitor ğŸ™ï¸

## ğŸ“Œ Introduction
Smart City Trash Bin Monitor is a **real-time data engineering project** that simulates and processes IoT-enabled trash bin data using a **fault-tolerant streaming architecture**.  
The project focuses on building a **production-grade streaming pipeline** that ingests sensor data, cleans and aggregates it in real time, and stores reliable results for downstream consumption.

This repository currently implements the **core real-time data pipeline** with strong guarantees around **performance, reliability, and maintainability**.

---

## ğŸ“– Project Description
Modern cities generate continuous streams of waste management data from smart bins deployed across wards and zones.  
This project demonstrates how such data can be:

- Ingested in real time
- Validated and cleaned safely
- Processed with **exactly-once semantics**
- Persisted reliably even during failures
- Scaled and maintained using best practices

The emphasis of this project is **data engineering correctness and robustness**, not just data movement.

---

## ğŸ¯ Objectives (Implemented)
- Real-time ingestion of trash bin sensor data
- Safe handling of malformed or invalid events
- Deduplication and late-data handling
- Ward-level aggregation of bin fill levels
- Reliable persistence with retry and recovery
- Environment-driven configuration (Docker-ready)

---

## ğŸ§  Key Features (Current Implementation)

### âœ… Real-Time Data Ingestion
- Kafka-based ingestion pipeline
- Controlled ingestion rate using `maxOffsetsPerTrigger`
- Separate handling for valid and invalid events

### âœ… Stream Processing with Spark Structured Streaming
- Stateful processing with watermarking
- Deduplication based on business keys
- Windowed aggregations (ward-wise fill levels)
- Exactly-once guarantees using checkpointing

### âœ… Dead Letter Queue (DLQ)
- Invalid or malformed events routed to a dedicated Kafka topic
- DLQ is isolated and does not block the main pipeline
- Full auditability of bad data

### âœ… Fault Tolerance & Recovery
- Safe `foreachBatch` execution
- Database retries with exponential backoff
- Automatic recovery from Spark restarts
- No duplicate writes due to idempotent UPSERTs

### âœ… Performance Optimized
- Batch time reduced from ~20s to ~2â€“6s
- Optimized Spark parallelism and shuffles
- Batched database writes

### âœ… Maintainable & Configurable
- All infrastructure config externalized via environment variables
- Schema versioning for forward compatibility
- Clean modular Spark job structure

---

## ğŸ—ï¸ Current Architecture (Implemented)

Data Simulator (Python)
â†“
Apache Kafka
â”œâ”€â”€ valid-trash-bin-data
â””â”€â”€ invalid-trash-bin-data (DLQ)
â†“
Apache Spark Structured Streaming
â†“
PostgreSQL (Aggregated Results)

---

## ğŸ§° Tech Stack (Implemented)

| Layer | Technology |
|-----|-----------|
| Data Simulation | Python |
| Streaming Ingestion | Apache Kafka |
| Stream Processing | Apache Spark Structured Streaming |
| Fault Handling | Kafka Dead Letter Queue |
| Data Storage | PostgreSQL |
| Containerization | Docker & Docker Compose |
| Language | Python |
| Observability | Spark StreamingQueryListener |

---

## ğŸ“‚ Project Structure (Current)

smart-city-trash-bin-monitor/
â”‚
â”œâ”€â”€ simulator/ # Trash bin data simulator
â”œâ”€â”€ spark-apps/ # Spark Structured Streaming job
â”‚ â”œâ”€â”€ kafka_to_postgres.py
â”‚ â”œâ”€â”€ config.py
â”‚ â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml # Kafka, Spark, Postgres setup
â”œâ”€â”€ .env.example # Environment configuration template
â””â”€â”€ README.md

---

## ğŸš€ How to Run (Current)

- git clone (https://github.com/AbhiSathya/smart_city_trash_bin_monitor.git)

- cd smart-city-trash-bin-monitor

- docker compose up --build


Spark will:

- Consume live Kafka data

- Process valid events

- Route invalid events to DLQ

- Persist aggregated results into PostgreSQL

---

## ğŸ§ª Failure Scenarios Handled
âœ… Invalid JSON â†’ routed to DLQ

âœ… Duplicate events â†’ deduplicated

âœ… Postgres temporarily down â†’ retried safely

âœ… Spark restart â†’ resumes from checkpoint

âœ… Late data â†’ handled via watermarking

---

## ğŸ”® Planned Enhancements (Not Implemented Yet)
The following features are intentionally not implemented yet and are planned as future phases:

ğŸ”² Backend API (FastAPI) for querying bin status

ğŸ”² Dashboard (Map & charts for monitoring)

ğŸ”² Alerting system (overflow thresholds)

ğŸ”² Route optimization & prediction logic

ğŸ”² Historical batch analytics

ğŸ”² Airflow-based orchestration