services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"

  kafka:
    image: bitnami/kafka:3.5
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports:
      - "9092:9092"

    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--bootstrap-server", "kafka:9092", "--list"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  simulator:
    build:
      context: ./simulator
    container_name: simulator
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - VALID_TOPIC=${VALID_TOPIC}
      - INVALID_TOPIC=${INVALID_TOPIC}
      - DATA_INTERVAL_SECONDS=${DATA_INTERVAL_SECONDS}
      - ERROR_FREQ=${ERROR_FREQ}
    restart: always

  consumer:
    build:
      context: ./consumer
    container_name: consumer
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - VALID_TOPIC=${VALID_TOPIC}
      - INVALID_TOPIC=${INVALID_TOPIC}
      - CONSUMER_GROUP_ID=${CONSUMER_GROUP_ID}
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
    restart: always

  spark:
    build:
      context: ./spark-apps
    container_name: spark
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started
    user: root
    volumes:
      - ./spark-apps:/opt/spark-apps
      - spark_checkpoints:/opt/spark-checkpoints
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /opt/spark-checkpoints/ward_fill_level_aggregation_v1
        chown -R spark:spark /opt/spark-checkpoints
        exec su spark -c "
          /opt/spark/bin/spark-submit \
          --master local[*] \
          --conf spark.jars.ivy=/tmp/.ivy2 \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.1 \
          /opt/spark-apps/kafka_to_postgres.py
        "
    ports:
      - "4040:4040" # Spark Web UI
    restart: "no" # IMPORTANT: Spark should NOT auto-restart on failure to avoid duplicate processing

  api:
    build:
      context: ./backend/api
    container_name: api
    depends_on:
      postgres:
        condition: service_started
    environment:
      DB_HOST: ${DB_HOST}
      DB_PORT: ${DB_PORT}
      DB_NAME: ${DB_NAME}
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
    ports:
      - "8000:8000"
    restart: unless-stopped

  redis:
    image: redis:7
    container_name: redis
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379
    ports:
      - "6379:6379"
    restart: unless-stopped


volumes:
  pgdata:
  spark_checkpoints:
